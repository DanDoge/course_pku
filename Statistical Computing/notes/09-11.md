## optimization

least square
- |X^TX| = 0 ? pseudo-inverse
- sparse regularization: beta^1(ridge), beta^2(Lasso)

general convex optimization
- min. f s.t. f_i <= 0, h_i = 0
- for all x, f(y) >= f(x) + deltaf(x)(y - x)
    - let x = Ex, let y be a RV, s.t. E(y) = Ex
    - f(EX) >= E(f(x))
- for unconstrainted porb. or internal points
    - x = x^* iff. deltaf(x) = 0
- local optimality
    - min. f s.t. f_i <= 0, h_i = 0 and |x - x_0| <= R
- Largrangian g(u, v) = inf L(X, u, v), inf of affine functions ==> concave
    - g(u, v) <= p^*
    - KKT contidions

max. entropy dist.
- X ~ N(a, b^2) is the max. ent. dist. s.t. EX = a, EX^2 = a^2 + b^2

change norm function to P
- optimal direction: -P^{-1}deltaf(x)
- P:= Hessian mat. ==> Newton's method
