{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "np.random.seed(1234)\n",
    "\n",
    "n, d = 100000, 100\n",
    "X = np.random.normal(size=(n,d))\n",
    "beta_0 = np.random.normal(size=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(X, beta, bias):\n",
    "    return 1.0 / (1.0 + np.exp(- bias - np.matmul(X, beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_label(X, beta, bias):\n",
    "    return np.random.binomial(1, get_prob(X, beta, bias))\n",
    "\n",
    "Y_gt = gen_label(X, beta_0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(X, Y_gt, Y_pred, batch_size):\n",
    "    grad_bias = np.sum(Y_gt - Y_pred)\n",
    "    grad_beta = np.dot(np.transpose(X), Y_gt - Y_pred)\n",
    "    return grad_bias / batch_size, grad_beta / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(X, Y_gt, prob):\n",
    "    res = 1.0\n",
    "    for i in range(X.shape[0]):\n",
    "        if Y_gt[i] == 1.0:\n",
    "            res *= prob[i]\n",
    "        else:\n",
    "            res *= 1.0 - prob[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(lr_init, step, decay):\n",
    "    warm_up_step = 100.0\n",
    "    lr_decay = 1e-6\n",
    "    if step <= warm_up_step:\n",
    "        return lr_init * step / warm_up_step\n",
    "    if not decay:\n",
    "        return lr_init\n",
    "    return np.power(1 - lr_decay, step - 100) * lr_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y_gt, start_ele, batch_size):\n",
    "    if start_ele + batch_size >= n:\n",
    "        X_batch = np.concatenate((X[start_ele: ], X[ :start_ele + batch_size - n]))\n",
    "        Y_gt_batch = np.concatenate((Y_gt[start_ele: ], Y_gt[ :start_ele + batch_size - n]))\n",
    "    else:\n",
    "        X_batch = X[start_ele: (start_ele + batch_size)]\n",
    "        Y_gt_batch = Y_gt[start_ele: (start_ele + batch_size)]\n",
    "    return X_batch, Y_gt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size = n\n",
    "def gd(X, Y_gt, lr_init, d):\n",
    "    start = time.time()\n",
    "    beta = np.zeros(shape=d)\n",
    "    bias = 0\n",
    "    step = 1\n",
    "    while True:\n",
    "        Y_pred = get_prob(X, beta, bias)\n",
    "        grad_bias, grad_beta = get_gradient(X, Y_gt, Y_pred, X.shape[0])\n",
    "        lr =  lr_scheduler(lr_init, step, 1)\n",
    "        beta += lr * grad_beta\n",
    "        bias += lr * grad_bias\n",
    "        step += 1\n",
    "        if np.sum(np.absolute(grad_beta)) + np.absolute(grad_bias) < 1e-3:\n",
    "            print(\"vanilla GD ended with L_1 diff as: \", np.sum(np.absolute(beta_0 - beta)) + np.absolute(bias))\n",
    "            print(\"Total time:\", time.time() - start, , \"total steps:\", step)\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nag(X, Y_gt, lr_init, d):\n",
    "    start = time.time()\n",
    "    beta = np.zeros(shape=d)\n",
    "    bias = 0\n",
    "    step = 1\n",
    "    beta_tmp = beta\n",
    "    bias_tmp = bias\n",
    "    while True:\n",
    "        y_beta = beta + ((step - 2.0) / (step + 1.0)) * (beta - beta_tmp)\n",
    "        y_bias = bias + ((step - 2.0) / (step + 1.0)) * (bias - bias_tmp)\n",
    "        Y_pred = get_prob(X, y_beta, y_bias)\n",
    "        grad_bias, grad_beta = get_gradient(X, Y_gt, Y_pred, X.shape[0])\n",
    "        lr =  lr_scheduler(lr_init, step, 1)\n",
    "        beta_tmp = beta\n",
    "        bias_tmp = bias\n",
    "        beta = y_beta + lr * grad_beta\n",
    "        bias = y_bias + lr * grad_bias\n",
    "        step += 1\n",
    "        if np.sum(np.absolute(grad_beta)) + np.absolute(grad_bias) < 1e-3:\n",
    "            print(\"NAG ended with L_1 diff as: \", np.sum(np.absolute(beta_0 - beta)) + np.absolute(bias))\n",
    "            print(\"Total time:\", time.time() - start, \"total steps:\", step)\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD ended with L_1 diff as:  1.331085643775021\n",
      "Total time: 142.7242739200592\n"
     ]
    }
   ],
   "source": [
    "gd(X, Y_gt, 0.2, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAG ended with L_1 diff as:  1.3035698905060682\n",
      "Total time: 1.8251197338104248\n"
     ]
    }
   ],
   "source": [
    "nag(X, Y_gt, 0.2, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(X, Y_gt, lr_init, d, eps, batch_size):\n",
    "    start = time.time()\n",
    "    beta = np.zeros(shape=d)\n",
    "    bias = 0\n",
    "    step = 1\n",
    "    g_beta = np.zeros(shape=d)\n",
    "    g_bias = eps\n",
    "    while True:\n",
    "        start_ele = ((step - 1) * batch_size) % n\n",
    "        X_batch, Y_gt_batch = get_batch(X, Y_gt, start_ele, batch_size)\n",
    "        Y_pred = get_prob(X_batch, beta, bias)\n",
    "        grad_bias, grad_beta = get_gradient(X_batch, Y_gt_batch, Y_pred, batch_size)\n",
    "        g_beta += np.square(grad_beta)\n",
    "        g_bias += grad_bias * grad_bias\n",
    "        lr =  lr_scheduler(lr_init, step, 0)\n",
    "        beta += lr * np.multiply((1.0 / np.sqrt(g_beta + eps)), grad_beta)\n",
    "        bias += lr * 1.0 / np.sqrt(g_bias + eps) * grad_bias\n",
    "        step += 1\n",
    "        if step > 1e5:\n",
    "            print(\"AdaGrad ended with L_1 diff as: \", np.sum(np.absolute(beta_0 - beta)) + np.absolute(bias))\n",
    "            print(\"Total time:\", time.time() - start)\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(X, Y_gt, lr_init, d, eps, batch_size):\n",
    "    start = time.time()\n",
    "    beta = np.zeros(shape=d)\n",
    "    bias = 0\n",
    "    step = 1\n",
    "    g_beta = np.zeros(shape=d)\n",
    "    g_bias = 0\n",
    "    while True:\n",
    "        start_ele = ((step - 1) * batch_size) % n\n",
    "        X_batch, Y_gt_batch = get_batch(X, Y_gt, start_ele, batch_size)\n",
    "        Y_pred = get_prob(X_batch, beta, bias)\n",
    "        grad_bias, grad_beta = get_gradient(X_batch, Y_gt_batch, Y_pred, batch_size)\n",
    "        g_beta = 0.9 * g_beta + 0.1 * np.square(grad_beta)\n",
    "        g_bias = 0.9 * g_bias + 0.1 * np.square(grad_bias)\n",
    "        lr =  lr_scheduler(lr_init, step, 0)\n",
    "        beta += lr * np.multiply((1.0 / np.sqrt(g_beta + eps)), grad_beta)\n",
    "        bias += lr * 1.0 / np.sqrt(g_bias + eps) * grad_bias\n",
    "        step += 1\n",
    "        if step > 1e5:\n",
    "            print(\"RMSprop ended with L_1 diff as: \", np.sum(np.absolute(beta_0 - beta)) + np.absolute(bias))\n",
    "            print(\"Total time:\", time.time() - start)\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, Y_gt, lr_init, d, batch_size):\n",
    "    start = time.time()\n",
    "    beta = np.zeros(shape=d)\n",
    "    bias = 0\n",
    "    step = 1\n",
    "    while True:\n",
    "        start_ele = ((step - 1) * batch_size) % n\n",
    "        X_batch, Y_gt_batch = get_batch(X, Y_gt, start_ele, batch_size)\n",
    "        Y_pred = get_prob(X_batch, beta, bias)\n",
    "        grad_bias, grad_beta = get_gradient(X_batch, Y_gt_batch, Y_pred, batch_size)\n",
    "        lr =  lr_scheduler(lr_init, step, 1)\n",
    "        beta += lr * grad_beta\n",
    "        bias += lr * grad_bias\n",
    "        step += 1\n",
    "        if step > 1e5:\n",
    "            print(\"SGD ended with L_1 diff as: \", np.sum(np.absolute(beta_0 - beta)) + np.absolute(bias))\n",
    "            print(\"Total time:\", time.time() - start)\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(X, Y_gt, lr_init, d, b_1, b_2, eps, batch_size):\n",
    "    start = time.time()\n",
    "    beta = np.zeros(shape=d)\n",
    "    bias = 0\n",
    "    step = 1\n",
    "    m_beta = np.zeros(shape=d)\n",
    "    m_bias = 0\n",
    "    v_beta = np.zeros(shape=d)\n",
    "    v_bias = 0\n",
    "    while True:\n",
    "        start_ele = ((step - 1) * batch_size) % n\n",
    "        X_batch, Y_gt_batch = get_batch(X, Y_gt, start_ele, batch_size)\n",
    "        Y_pred = get_prob(X_batch, beta, bias)\n",
    "        grad_bias, grad_beta = get_gradient(X_batch, Y_gt_batch, Y_pred, batch_size)\n",
    "        lr =  lr_scheduler(lr_init, step, 0)\n",
    "        m_beta = b_1 * m_beta + (1 - b_1) * grad_beta\n",
    "        v_beta = b_2 * v_beta + (1 - b_2) * np.square(grad_beta)\n",
    "        m_bias = b_1 * m_bias + (1 - b_1) * grad_bias\n",
    "        v_bias = b_2 * v_bias + (1 - b_2) * np.square(grad_bias)\n",
    "        beta += lr * (m_beta / (1.0 - np.power(b_1, step))) / np.sqrt(eps + v_beta / (1.0 - np.power(b_2, step)))\n",
    "        bias += lr * (m_bias / (1.0 - np.power(b_1, step))) / np.sqrt(eps + v_bias / (1.0 - np.power(b_2, step)))\n",
    "        step += 1\n",
    "        if step > 1e5:\n",
    "            print(\"Adam ended with L_1 diff as: \", np.sum(np.absolute(beta_0 - beta)) + np.absolute(bias))\n",
    "            print(\"Total time:\", time.time() - start)\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD ended with L_1 diff as:  9.09050144158339\n",
      "Total time: 2.7672417163848877\n",
      "SGD ended with L_1 diff as:  9.123420527845402\n",
      "Total time: 4.467304468154907\n",
      "SGD ended with L_1 diff as:  9.137121484117696\n",
      "Total time: 5.595751762390137\n"
     ]
    }
   ],
   "source": [
    "sgd(X, Y_gt, 0.01, d, 32)\n",
    "sgd(X, Y_gt, 0.01, d, 64)\n",
    "sgd(X, Y_gt, 0.01, d, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaGrad ended with L_1 diff as:  38.2832443231089\n",
      "Total time: 3.361721992492676\n",
      "AdaGrad ended with L_1 diff as:  33.225706256670485\n",
      "Total time: 5.151470184326172\n",
      "AdaGrad ended with L_1 diff as:  28.493608711657178\n",
      "Total time: 6.045389890670776\n"
     ]
    }
   ],
   "source": [
    "adagrad(X, Y_gt, 0.01, d, 1e-8, 32)\n",
    "adagrad(X, Y_gt, 0.01, d, 1e-8, 64)\n",
    "adagrad(X, Y_gt, 0.01, d, 1e-8, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSprop ended with L_1 diff as:  10.034007078094094\n",
      "Total time: 3.659581422805786\n",
      "RMSprop ended with L_1 diff as:  7.059311638887635\n",
      "Total time: 5.650357246398926\n",
      "RMSprop ended with L_1 diff as:  5.162599348000608\n",
      "Total time: 6.600702285766602\n"
     ]
    }
   ],
   "source": [
    "rmsprop(X, Y_gt, 0.01, d, 1e-8, 32)\n",
    "rmsprop(X, Y_gt, 0.01, d, 1e-8, 64)\n",
    "rmsprop(X, Y_gt, 0.01, d, 1e-8, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam ended with L_1 diff as:  8.64105172133137\n",
      "Total time: 4.710510969161987\n",
      "Adam ended with L_1 diff as:  6.397844412972676\n",
      "Total time: 7.259472846984863\n",
      "Adam ended with L_1 diff as:  4.868890311291243\n",
      "Total time: 8.804412841796875\n"
     ]
    }
   ],
   "source": [
    "adam(X, Y_gt, 0.01, d, 0.9, 0.999, 1e-8, 32)\n",
    "adam(X, Y_gt, 0.01, d, 0.9, 0.999, 1e-8, 64)\n",
    "adam(X, Y_gt, 0.01, d, 0.9, 0.999, 1e-8, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "sparse_rate = 0.3\n",
    "M = np.random.uniform(size=(n,d)) < sparse_rate\n",
    "X[M] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD ended with L_1 diff as:  57.65731843611058\n",
      "Total time: 2.8184800148010254\n",
      "SGD ended with L_1 diff as:  57.68765250100164\n",
      "Total time: 4.58999490737915\n",
      "SGD ended with L_1 diff as:  57.69437401595899\n",
      "Total time: 5.486193895339966\n"
     ]
    }
   ],
   "source": [
    "sgd(X, Y_gt, 0.01, d, 32)\n",
    "sgd(X, Y_gt, 0.01, d, 64)\n",
    "sgd(X, Y_gt, 0.01, d, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaGrad ended with L_1 diff as:  58.19815381513825\n",
      "Total time: 3.4891746044158936\n",
      "AdaGrad ended with L_1 diff as:  57.84116505523259\n",
      "Total time: 5.4762866497039795\n",
      "AdaGrad ended with L_1 diff as:  57.72334261079924\n",
      "Total time: 6.256183862686157\n"
     ]
    }
   ],
   "source": [
    "adagrad(X, Y_gt, 0.01, d, 1e-8, 32)\n",
    "adagrad(X, Y_gt, 0.01, d, 1e-8, 64)\n",
    "adagrad(X, Y_gt, 0.01, d, 1e-8, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSprop ended with L_1 diff as:  56.06149062342203\n",
      "Total time: 3.677885055541992\n",
      "RMSprop ended with L_1 diff as:  56.48542304910004\n",
      "Total time: 5.7852911949157715\n",
      "RMSprop ended with L_1 diff as:  56.834935073267054\n",
      "Total time: 6.378304481506348\n"
     ]
    }
   ],
   "source": [
    "rmsprop(X, Y_gt, 0.01, d, 1e-8, 32)\n",
    "rmsprop(X, Y_gt, 0.01, d, 1e-8, 64)\n",
    "rmsprop(X, Y_gt, 0.01, d, 1e-8, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam ended with L_1 diff as:  56.41720083122279\n",
      "Total time: 4.750304698944092\n",
      "Adam ended with L_1 diff as:  56.5946042427963\n",
      "Total time: 7.820711851119995\n",
      "Adam ended with L_1 diff as:  56.85074832819873\n",
      "Total time: 8.74868392944336\n"
     ]
    }
   ],
   "source": [
    "adam(X, Y_gt, 0.01, d, 0.9, 0.999, 1e-8, 32)\n",
    "adam(X, Y_gt, 0.01, d, 0.9, 0.999, 1e-8, 64)\n",
    "adam(X, Y_gt, 0.01, d, 0.9, 0.999, 1e-8, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
