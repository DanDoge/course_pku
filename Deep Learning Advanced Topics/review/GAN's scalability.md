A seemingly more brute-forcing way to train GAN for high-resolution synthesis is to train them at large scale. In BigGAN, it has been shown that GAN's performance simply increases as channel numbers and batch sizes grow larger. GAN behaviors at large scale are also studied, including failure when truncation trick applied, mode collapse which only studied at a smaller scale. Several tradeoffs have been found when tackling these issues: the tradeoff between fidelity and variety, between image quality and training stability.