Given a content and a style image, style transfer aims to synthesize an output image which combines the content and style from reference images. A general approach is to encode input images to some latent space, and synthesize output from these latent vectors. Inspired by AdaIn, various methods combines these vectors via aligning the distribution of pixels by adjusting the scale and variance of atcivation in a neural network. [] proposed a closed-form solution using technics from optimal transport. Also, due to lack of paired training data, style transfer methods often adapt their own approach to enforce the separation of content and style, like VGG loss, minimizing cycle loss or GAN loss.
